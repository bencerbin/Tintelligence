{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ac48da",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f682fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f6e8881-fddc-412b-9e87-ed20a80721ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes github ignore all the data\n",
    "\n",
    "with open(\".gitignore\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "# Ignore image data folders\n",
    "tiny-imagenet-200/\n",
    "tiny-imagenet-200-grayscale/\n",
    "\n",
    "# Ignore any .DS_Store or similar files\n",
    ".DS_Store\n",
    "\n",
    "# Ignore Python cache files\n",
    "__pycache__/\n",
    "*.pyc\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126b053",
   "metadata": {},
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21736c21-6bf8-4d8f-af68-d7f5323581d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted.\n"
     ]
    }
   ],
   "source": [
    "# Download download Tiny ImageNet directly into your Jupyter Notebook\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# set directory and download URL\n",
    "url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "output_dir = \"tiny-imagenet-200\"\n",
    "zip_filename = \"tiny-imagenet-200.zip\"\n",
    "\n",
    "# download zip file if it doesn't exist\n",
    "if not os.path.exists(zip_filename):\n",
    "    print(\"Downloading Tiny ImageNet...\")\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(zip_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# unzip if not already extracted\n",
    "if not os.path.exists(output_dir):\n",
    "    print(\"Extracting zip file...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Already extracted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95a5944d-c96f-4b3c-b302-eee38a478e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ac54ccf7a74a1d98fbe41f8535f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting images:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing: val\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d90d354ee3e498986e2e4407d6953a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting images:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing: test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518c4faec8044c77bf35dd5ce3b91171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting images:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_jpegs_to_bw(source_root, dest_root):\n",
    "    source_root = Path(source_root)\n",
    "    dest_root = Path(dest_root)\n",
    "\n",
    "    dest_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Collect all image paths first\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(source_root):\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".jpeg\".upper(), \".jpg\".upper())):\n",
    "                all_files.append((root, file))\n",
    "\n",
    "    # Convert with progress bar\n",
    "    for root, file in tqdm(all_files, desc=\"Converting images\"):\n",
    "        source_path = Path(root) / file\n",
    "        relative_path = source_path.relative_to(source_root)\n",
    "        dest_path = dest_root / relative_path\n",
    "\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            img = Image.open(source_path).convert(\"L\")\n",
    "            img.save(dest_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {source_path}: {e}\")\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for split in splits:\n",
    "    source = f\"tiny-imagenet-200/{split}\"\n",
    "    dest = f\"tiny-imagenet-200-grayscale/{split}\"\n",
    "    print(f\"\\n📂 Processing: {split}\")\n",
    "    convert_jpegs_to_bw(source, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c3b2937-f800-49d3-8959-726653b294ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jmpb2020/Tintelligence/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37588791-8392-4bf6-8f89-80743ec07182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec8c177470d4409a869db99b60a6e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting images:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_folder = \"tiny-imagenet-200/train\"\n",
    "destination_folder = \"tiny-imagenet-200-grayscale/train\"\n",
    "\n",
    "convert_jpegs_to_bw(source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53c55a9b-f0ea-421f-88b7-1503f293743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose a sample image from Tiny ImageNet\n",
    "# sample_img_path = os.path.join(\"tiny-imagenet-200\", \"train\", \"n01443537\", \"images\")\n",
    "# sample_img = os.listdir(sample_img_path)[0]\n",
    "# full_img_path = os.path.join(sample_img_path, sample_img)\n",
    "\n",
    "# # Load images\n",
    "# img_color = Image.open(full_img_path).convert('RGB')\n",
    "# img_gray = img_color.convert('L')\n",
    "\n",
    "# # Plot side by side\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# axs[0].imshow(img_color)\n",
    "# axs[0].set_title(\"Original Color\")\n",
    "# axs[0].axis('off')\n",
    "\n",
    "# axs[1].imshow(img_gray, cmap='gray')\n",
    "# axs[1].set_title(\"Grayscale Version\")\n",
    "# axs[1].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89474e47-8c73-4c9b-993a-3f84954b934b",
   "metadata": {},
   "source": [
    "## Dataset Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d020e80-faad-41d6-aed0-fd8a8c1d2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, grayscale_dir, color_dir, transform_input=None, transform_target=None):\n",
    "        self.grayscale_dir = grayscale_dir\n",
    "        self.color_dir = color_dir\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_target = transform_target\n",
    "        self.samples = []\n",
    "\n",
    "        for class_name in os.listdir(grayscale_dir):\n",
    "            gray_class_path = os.path.join(grayscale_dir, class_name, \"images\")\n",
    "            color_class_path = os.path.join(color_dir, class_name, \"images\")\n",
    "\n",
    "            if not os.path.isdir(gray_class_path):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(gray_class_path):\n",
    "                gray_img_path = os.path.join(gray_class_path, img_name)\n",
    "                color_img_path = os.path.join(color_class_path, img_name)\n",
    "\n",
    "                if os.path.exists(color_img_path):\n",
    "                    self.samples.append((gray_img_path, color_img_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gray_path, color_path = self.samples[idx]\n",
    "        gray = Image.open(gray_path).convert(\"L\")\n",
    "        color = Image.open(color_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform_input:\n",
    "            gray = self.transform_input(gray)\n",
    "        if self.transform_target:\n",
    "            color = self.transform_target(color)\n",
    "\n",
    "        return gray, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f315f92-a9ed-488a-9336-75eaf7037862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colorization_loaders(grayscale_path, color_path, batch_size=32):\n",
    "    transform_input = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform_target = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = ColorizationDataset(grayscale_path, color_path, transform_input, transform_target)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74ada792-895b-4fdd-9831-8cd120103a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/jmpb2020/Tintelligence/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f729ddcb-6da2-435b-8726-6a5d693e3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale Directory Exists: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Grayscale Directory Exists:\", os.path.exists(\"tiny-imagenet-200-grayscale/train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52dd9a38-d8a9-4af3-8da6-9c6249282c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale root exists: True\n",
      "Grayscale train exists: True\n",
      "Sample class folders: ❌ No /train folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Grayscale root exists:\", os.path.exists(\"tiny-imagenet-200-grayscale\"))\n",
    "print(\"Grayscale train exists:\", os.path.exists(\"tiny-imagenet-200-grayscale/train\"))\n",
    "print(\"Sample class folders:\", os.listdir(\"../tiny-imagenet-200-grayscale/train\")[:5] if os.path.exists(\"../tiny-imagenet-200-grayscale/train\") else \"❌ No /train folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c40c29f-a1ec-4344-85a4-bb2c089f61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_path = \"./tiny-imagenet-200-grayscale/train\"\n",
    "color_path = \"./tiny-imagenet-200/train\"\n",
    "train_loader = get_colorization_loaders(grayscale_path, color_path, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f39bbb17-6b22-439a-a1bb-85f780ae0a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'val', 'test']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"tiny-imagenet-200-grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b14d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cifar10_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "#     # TINT TODO: Change this to utilize some of the training progress we made in bw_colorizer\n",
    "\n",
    "#     # Data specific transforms\n",
    "#     data_std = (0.2470, 0.2435, 0.2616)\n",
    "#     data_mean = (0.4914, 0.4822, 0.4465)\n",
    "#     xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n",
    "\n",
    "#     # Training dataset and loader\n",
    "#     train_dataset = CIFAR10(root=path, train=True, download=True, transform=xforms)\n",
    "\n",
    "#     # Set the batch size to N if batch_size is 0\n",
    "#     tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "#     valid_dataset = CIFAR10(root=path, train=False, download=True, transform=xforms)\n",
    "\n",
    "#     # Set the batch size to N if batch_size is 0\n",
    "#     vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "#     valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "#     return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd7c5a",
   "metadata": {},
   "source": [
    "## Training Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24a9968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(mb, loader, device, model, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    dataiterator = iter(loader)\n",
    "\n",
    "    for batch in progress_bar(range(num_batches), parent=mb):\n",
    "\n",
    "        mb.child.comment = \"Training\"\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        X, Y = next(dataiterator)\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "        # Compute the output\n",
    "        output = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, Y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73463196",
   "metadata": {},
   "source": [
    "## Validation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "103de938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(mb, loader, device, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    num_correct = 0\n",
    "\n",
    "    num_classes = len(loader.dataset.classes)\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "\n",
    "    N = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    dataiterator = iter(loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batches = range(num_batches)\n",
    "        batches = progress_bar(batches, parent=mb) if mb else batches\n",
    "        for batch in batches:\n",
    "\n",
    "            if mb:\n",
    "                mb.child.comment = f\"Validation\"\n",
    "\n",
    "            # Grab the batch of data and send it to the correct device\n",
    "            X, Y = next(dataiterator)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            loss = criterion(output, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = output.argmax(dim=1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            comparisons = predictions == Y\n",
    "            num_correct += comparisons.type(torch.float).sum().item()\n",
    "\n",
    "            # Sum up number of correct per class\n",
    "            for result, clss in zip(comparisons, Y):\n",
    "                class_correct[clss] += result.item()\n",
    "                class_total[clss] += 1\n",
    "\n",
    "    accuracy = 100 * (num_correct / N)\n",
    "    accuracies = {\n",
    "        clss: 100 * class_correct[clss] / class_total[clss]\n",
    "        for clss in range(num_classes)\n",
    "    }\n",
    "\n",
    "    return losses, accuracy, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11651d",
   "metadata": {},
   "source": [
    "## Loss Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2ac9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots(mb, train_losses, valid_losses, epoch, num_epochs):\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    train_xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(0, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[train_xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041f60d",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e18e3af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device.\n"
     ]
    }
   ],
   "source": [
    "# TODO: tune the training batch size\n",
    "train_batch_size = 128\n",
    "\n",
    "# Let's use some shared space for the data (so that we don't have copies\n",
    "# sitting around everywhere)\n",
    "data_path = \"~/data\"\n",
    "\n",
    "# Use the GPUs if they are available\n",
    "# TODO: if you run into GPU memory errors you should set device to \"cpu\" and restart the notebook\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "valid_batch_size = 5000\n",
    "grayscale_path = \"./tiny-imagenet-200-grayscale/train\"\n",
    "color_path = \"./tiny-imagenet-200/train\"\n",
    "train_loader = get_colorization_loaders(grayscale_path, color_path, batch_size=8)\n",
    "\n",
    "# Input and output sizes depend on data\n",
    "class_names = sorted(os.listdir(grayscale_path))\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b13ca31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab a bunch of images and change the range to [0, 1]\n",
    "# nprint = 64\n",
    "# images = torch.tensor(train_loader.dataset.data[:nprint] / 255)\n",
    "# targets = train_loader.dataset.targets[:nprint]\n",
    "# labels = [f\"{class_names[target]:>10}\" for target in targets]\n",
    "\n",
    "# # Create a grid of the images (make_grid expects (BxCxHxW))\n",
    "# image_grid = make_grid(images.permute(0, 3, 1, 2))\n",
    "\n",
    "# _, ax = plt.subplots(figsize=(16, 16))\n",
    "# ax.imshow(image_grid.permute(1, 2, 0))\n",
    "# ax.grid(None)\n",
    "\n",
    "# images_per_row = int(nprint ** 0.5)\n",
    "# for row in range(images_per_row):\n",
    "#     start_index = row * images_per_row\n",
    "#     print(\" \".join(labels[start_index : start_index + images_per_row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d526f6",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b57cc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges an image into a column vector\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        # TODO: add dropout and/or batch normalization\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer, *hidden_layers, output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1616b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # TODO: create layers here\n",
    "\n",
    "        # Early CNNs had the following structure:\n",
    "        #    X -> [[Conv2d -> ReLU] x N -> MaxPool2d] x M\n",
    "        #      -> [Linear -> ReLU] x K -> Linear\n",
    "        #   Where\n",
    "        #     0 ≤ N ≤ 3\n",
    "        #     0 ≤ M ≤ 3\n",
    "        #     0 ≤ K < 3\n",
    "        #\n",
    "        # The \"[[Conv2d -> ReLU] x N -> MaxPool2d] x M\" part extracts\n",
    "        # useful features, and the \"[Linear -> ReLU] x K -> Linear\" part\n",
    "        # performs the classification.\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Conv layer 1: (3 input channels, 32 output channels)\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2\n",
    "\n",
    "            # Conv layer 2: (32 input channels, 64 output channels)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample again\n",
    "        )\n",
    "\n",
    "        # After two max-pool layers on 32x32 input → 8x8 feature maps\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),  # flatten the output from conv layers\n",
    "            nn.Linear(64 * 8 * 8, 128),  # fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)  # 10 output classes for CIFAR-10\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # TODO: implement forward pass here\n",
    "        X = self.conv_layers(X)\n",
    "        X = self.fc_layers(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "289e3507-b5c1-42dd-965d-4c9a9d52bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Simple CNN\n",
    "class ColorizationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),  # grayscale input\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 2, stride=2),  # 3-channel output\n",
    "            nn.Sigmoid()  # values in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c5d34fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       640\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       73,856\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─ConvTranspose2d: 2-7              32,832\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─ConvTranspose2d: 2-9              771\n",
      "|    └─Sigmoid: 2-10                     --\n",
      "=================================================================\n",
      "Total params: 108,099\n",
      "Trainable params: 108,099\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO: try out different network widths and depths\n",
    "# neurons_per_hidden_layer = [1024, 512, 256]\n",
    "# layer_sizes = [num_features, *neurons_per_hidden_layer, num_classes]\n",
    "# model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "# TODO: complete the CNN class in the cell above this one and then uncomment this line\n",
    "# model = CNN().to(device)\n",
    "\n",
    "# TODO: use an off-the-shell model from PyTorch\n",
    "# from torchvision.models import ...\n",
    "# model = ...\n",
    "\n",
    "# TINT TODO: make the output have 3 nodes.\n",
    "# from torchvision.models import resnet18\n",
    "# model = resnet18(num_classes=num_classes).to(device)\n",
    "\n",
    "# summary(model)\n",
    "\n",
    "#TINT\n",
    "# Instantiate the model\n",
    "model = ColorizationCNN().to(device)\n",
    "summary(model)\n",
    "# pixel-wise loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# TODO: try out different Adam hyperparameters\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c38b707-75ea-4233-ab14-5212da8c7acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test single batch\n",
    "\n",
    "gray_batch, color_batch = next(iter(train_loader))\n",
    "gray_batch, color_batch = gray_batch.to(device), color_batch.to(device)\n",
    "output = model(gray_batch)\n",
    "print(\"Output shape:\", output.shape)  # Should be [batch_size, 3, 64, 64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd179cb",
   "metadata": {},
   "source": [
    "## Training and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcd7b615",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m mb\u001b[38;5;241m.\u001b[39mmain_bar\u001b[38;5;241m.\u001b[39mcomment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Loss and accuracy prior to training\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m vl, accuracy, _ \u001b[38;5;241m=\u001b[39m validate(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[43mvalid_loader\u001b[49m, device, model, criterion)\n\u001b[1;32m     15\u001b[0m valid_losses\u001b[38;5;241m.\u001b[39mextend(vl)\n\u001b[1;32m     16\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: tune the number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "accuracies = []\n",
    "\n",
    "# A master bar for fancy output progress\n",
    "mb = master_bar(range(num_epochs))\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "mb.main_bar.comment = f\"Epochs\"\n",
    "\n",
    "# Loss and accuracy prior to training\n",
    "vl, accuracy, _ = validate(None, valid_loader, device, model, criterion)\n",
    "valid_losses.extend(vl)\n",
    "accuracies.append(accuracy)\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    tl = train_one_epoch(mb, train_loader, device, model, criterion, optimizer)\n",
    "    train_losses.extend(tl)\n",
    "\n",
    "    vl, accuracy, acc_by_class = validate(mb, valid_loader, device, model, criterion)\n",
    "    valid_losses.extend(vl)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    update_plots(mb, train_losses, valid_losses, epoch, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, '--o')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks(range(num_epochs+1))\n",
    "plt.ylim([0, 100])\n",
    "\n",
    "max_name_len = max(len(name) for name in class_names)\n",
    "\n",
    "print(\"Accuracy per class\")\n",
    "for clss in acc_by_class:\n",
    "    class_name = class_names[clss]\n",
    "    class_accuracy = acc_by_class[clss]\n",
    "    print(f\"  {class_name:>{max_name_len+2}}: {class_accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3681fc-9d47-45f6-b8d2-c9433b60c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "y_preds = []\n",
    "model.to(device)\n",
    "for x, y in valid_loader:\n",
    "    y_trues.append(y.cpu())\n",
    "    y_preds.append(model(x.to(device)).argmax(dim=1).cpu())\n",
    "\n",
    "y_true = torch.hstack(y_trues)\n",
    "y_pred = torch.hstack(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8a494-fd98-483a-b792-98ff40b05af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names).plot();\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58043e-1b64-4f21-93e0-4c744bfbc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take the three outputs and reconstruct an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc6623-85b8-4dbb-9b11-9116b6ba874b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
